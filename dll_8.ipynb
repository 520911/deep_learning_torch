{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-23T06:42:37.159932Z",
          "iopub.execute_input": "2023-06-23T06:42:37.160708Z",
          "iopub.status.idle": "2023-06-23T06:42:41.730402Z",
          "shell.execute_reply.started": "2023-06-23T06:42:37.160677Z",
          "shell.execute_reply": "2023-06-23T06:42:41.729329Z"
        },
        "trusted": true,
        "id": "rNS-uBSTrbLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.manythings.org/anki/rus-eng.zip\n",
        "!unzip rus-eng.zip\n",
        "!tail rus.txt"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-23T06:42:41.733163Z",
          "iopub.execute_input": "2023-06-23T06:42:41.736020Z",
          "iopub.status.idle": "2023-06-23T06:42:46.598777Z",
          "shell.execute_reply.started": "2023-06-23T06:42:41.735987Z",
          "shell.execute_reply": "2023-06-23T06:42:46.597569Z"
        },
        "trusted": true,
        "id": "z1jGyHXZrbLj",
        "outputId": "2e5b0233-f23e-4a27-bab8-6b4346ca2904"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "--2023-06-23 06:42:43--  https://www.manythings.org/anki/rus-eng.zip\nResolving www.manythings.org (www.manythings.org)... 173.254.30.110\nConnecting to www.manythings.org (www.manythings.org)|173.254.30.110|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 15460248 (15M) [application/zip]\nSaving to: ‘rus-eng.zip’\n\nrus-eng.zip         100%[===================>]  14.74M  50.6MB/s    in 0.3s    \n\n2023-06-23 06:42:44 (50.6 MB/s) - ‘rus-eng.zip’ saved [15460248/15460248]\n\nArchive:  rus-eng.zip\n  inflating: rus.txt                 \n  inflating: _about.txt              \nWe need to uphold laws against discrimination — in hiring, and in housing, and in education, and in the criminal justice system. That is what our Constitution and our highest ideals require.\tНам нужно отстаивать законы против дискриминации при найме на работу, в жилищной сфере, в сфере образования и правоохранительной системе. Этого требуют наша Конституция и высшие идеалы.\tCC-BY 2.0 (France) Attribution: tatoeba.org #5762728 (BHO) & #6390439 (odexed)\nI've heard that you should never date anyone who is less than half your age plus seven. Tom is now 30 years old and Mary is 17. How many years will Tom need to wait until he can start dating Mary?\tЯ слышал, что никогда не следует встречаться с кем-то вдвое младше вас плюс семь лет. Тому 30 лет, a Мэри 17. Сколько лет Тому нужно ждать до тех пор, пока он сможет начать встречаться с Мэри?\tCC-BY 2.0 (France) Attribution: tatoeba.org #10068197 (CK) & #10644473 (notenoughsun)\nI do have one final ask of you as your president, the same thing I asked when you took a chance on me eight years ago. I'm asking you to believe, not in my ability to bring about change but in yours.\tУ меня же, как у вашего президента, есть к вам последняя просьба. Та же самая, что и восемь лет назад, когда вы оказали мне своё доверие. Я прошу вас верить, но не в мои способности добиться перемен, а в ваши.\tCC-BY 2.0 (France) Attribution: tatoeba.org #5762723 (BHO) & #6390123 (odexed)\nIn today's world, we have to equip all our kids with an education that prepares them for success, regardless of what they look like, or how much their parents make, or the zip code that they live in.\tВ современном мире перед нами стоит задача дать всем нашим детям такое образование, которое настроит их на успех вне зависимости от того, как они выглядят, сколько зарабатывают их родители или какой у них почтовый индекс.\tCC-BY 2.0 (France) Attribution: tatoeba.org #3924477 (BHO) & #5968115 (odexed)\nDeath is something that we're often discouraged to talk about or even think about, but I've realized that preparing for death is one of the most empowering things you can do. Thinking about death clarifies your life.\tСмерть - это зачастую то, разговоры или даже мысли о чем приводят в уныние, но я осознал, что готовность умереть наделяет силой, как ничто другое. Мысль о смерти вносит ясность в твою жизнь.\tCC-BY 2.0 (France) Attribution: tatoeba.org #1969892 (davearms) & #3231553 (kukla)\nAt a moment when our economy is growing, our businesses are creating jobs at the fastest pace since the 1990s, and wages are starting to rise again, we have to make some choices about the kind of country we want to be.\tВ тот момент, когда наша экономика растёт, наши предприятия создают рабочие места наибольшими темпами, начиная с 90-х годов, а зарплаты снова начинают расти, мы должны принять ряд решений относительно того, какой страной мы хотим быть.\tCC-BY 2.0 (France) Attribution: tatoeba.org #3924474 (BHO) & #4509418 (odexed)\nWhen I was younger, I hated going to weddings. My grandmothers and aunts would huddle around me, poke me in the side, and giggle \"You're next! You're next!\" They only stopped this nonsense when I began to do the same thing at funerals.\tКогда я была помоложе, я ненавидела ходить на свадьбы. Мои бабушки и тётки толпились вокруг, тыкали меня в бок и говорили, посмеиваясь: «Ты следующая! Ты следующая!». Они перестали нести этот вздор только тогда, когда я начала делать то же самое на похоронах.\tCC-BY 2.0 (France) Attribution: tatoeba.org #2776770 (AlanF_US) & #4311406 (odexed)\nSince there are usually multiple websites on any given topic, I usually just click the back button when I arrive on any webpage that has pop-up advertising. I just go to the next page found by Google and hope for something less irritating.\tПоскольку сайтов, посвящённых какой-либо теме, как правило, несколько, я обычно просто нажимаю на кнопку \"назад\", если попадаю на страницу со всплывающей рекламой. Я просто перехожу на следующую страницу, найденную гуглом, и надеюсь найти что-то менее раздражающее.\tCC-BY 2.0 (France) Attribution: tatoeba.org #954270 (CK) & #6383010 (odexed)\nIf someone who doesn't know your background says that you sound like a native speaker, it means they probably noticed something about your speaking that made them realize you weren't a native speaker. In other words, you don't really sound like a native speaker.\tЕсли кто-то незнакомый говорит, что вы говорите как носитель языка, это значит, что он, вероятно, заметил что-то в вашей речи, что дало ему понять, что вы не носитель. Другими словами, вы не говорите как носитель.\tCC-BY 2.0 (France) Attribution: tatoeba.org #953936 (CK) & #10644468 (notenoughsun)\nDoubtless there exists in this world precisely the right woman for any given man to marry and vice versa; but when you consider that a human being has the opportunity of being acquainted with only a few hundred people, and out of the few hundred that there are but a dozen or less whom he knows intimately, and out of the dozen, one or two friends at most, it will easily be seen, when we remember the number of millions who inhabit this world, that probably, since the earth was created, the right man has never yet met the right woman.\tНесомненно, для каждого мужчины в этом мире где-то есть подходящая женщина, которая может стать ему женой, обратное верно и для женщин. Но если учесть, что у человека может быть максимум несколько сотен знакомых, из которых лишь дюжина, а то и меньше, тех, кого он знает близко, а из этой дюжины у него один или от силы два друга, то можно легко увидеть, что с учётом миллионов живущих на Земле людей, ни один подходящий мужчина, возможно, ещё не встретил подходящую женщину.\tCC-BY 2.0 (France) Attribution: tatoeba.org #7697649 (RM) & #7730831 (odexed)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "\n",
        "class Lang:\n",
        "  def __init__(self, name):\n",
        "    self.name = name\n",
        "    self.word2index = {}\n",
        "    self.word2count = {}\n",
        "    self.index2word = {0: 'SOS', 1: 'EOS'}\n",
        "    self.n_words = 2\n",
        "\n",
        "  def add_sentence(self, sentence):\n",
        "    for word in sentence.split(' '):\n",
        "      self.add_word(word)\n",
        "\n",
        "  def add_word(self, word):\n",
        "    if word not in self.word2index:\n",
        "      self.word2index[word] = self.n_words\n",
        "      self.word2count[word] = 1\n",
        "      self.index2word[self.n_words] = word\n",
        "      self.n_words += 1\n",
        "    else:\n",
        "      self.word2count[word] += 1"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-23T06:42:46.601034Z",
          "iopub.execute_input": "2023-06-23T06:42:46.601451Z",
          "iopub.status.idle": "2023-06-23T06:42:46.610794Z",
          "shell.execute_reply.started": "2023-06-23T06:42:46.601409Z",
          "shell.execute_reply": "2023-06-23T06:42:46.609848Z"
        },
        "trusted": true,
        "id": "FT9JT3rDrbLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# http://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Zа-яА-Я.!?]+\", r\" \", s)\n",
        "    return s"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-23T06:42:46.613657Z",
          "iopub.execute_input": "2023-06-23T06:42:46.614638Z",
          "iopub.status.idle": "2023-06-23T06:42:46.623481Z",
          "shell.execute_reply.started": "2023-06-23T06:42:46.614604Z",
          "shell.execute_reply": "2023-06-23T06:42:46.622489Z"
        },
        "trusted": true,
        "id": "XpfULN5xrbLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_lang(lang1='rus', lang2='eng', reverse=False):\n",
        "  with open('rus.txt', encoding='utf-8') as f:\n",
        "    data = f.readlines()\n",
        "\n",
        "  pairs = [[normalizeString(j) for j in i.strip().split('\\t')[:2]] for i in data]\n",
        "\n",
        "  if reverse:\n",
        "    pairs = [list(reversed(p)) for p in pairs]\n",
        "    input_lang = Lang(lang2)\n",
        "    output_lang = Lang(lang1)\n",
        "  else:\n",
        "    input_lang = Lang(lang1)\n",
        "    output_lang = Lang(lang2)\n",
        "  return input_lang, output_lang, pairs"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-23T06:42:46.625262Z",
          "iopub.execute_input": "2023-06-23T06:42:46.625682Z",
          "iopub.status.idle": "2023-06-23T06:42:46.634590Z",
          "shell.execute_reply.started": "2023-06-23T06:42:46.625649Z",
          "shell.execute_reply": "2023-06-23T06:42:46.633655Z"
        },
        "trusted": true,
        "id": "Ebodh_WyrbLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 10\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s\",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "        p[0].startswith(eng_prefixes)\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-23T06:42:46.636318Z",
          "iopub.execute_input": "2023-06-23T06:42:46.636774Z",
          "iopub.status.idle": "2023-06-23T06:42:46.644760Z",
          "shell.execute_reply.started": "2023-06-23T06:42:46.636739Z",
          "shell.execute_reply": "2023-06-23T06:42:46.643569Z"
        },
        "trusted": true,
        "id": "OgGtv45CrbLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = read_lang(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.add_sentence(pair[0])\n",
        "        output_lang.add_sentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'rus')\n",
        "print(random.choice(pairs))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-23T06:42:46.646435Z",
          "iopub.execute_input": "2023-06-23T06:42:46.646751Z",
          "iopub.status.idle": "2023-06-23T06:43:10.008512Z",
          "shell.execute_reply.started": "2023-06-23T06:42:46.646722Z",
          "shell.execute_reply": "2023-06-23T06:43:10.007388Z"
        },
        "trusted": true,
        "id": "CYGSA9gfrbLm",
        "outputId": "7fedc2ba-ead7-43e8-8ff3-ad5515b84b84"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Read 467119 sentence pairs\nTrimmed to 27228 sentence pairs\nCounting words...\nCounted words:\neng 4229\nrus 9939\n['i m not sure tom is ready .', 'я не уверен что том готов .']\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder"
      ],
      "metadata": {
        "id": "vPmZFV4_rbLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, rnn_type, num_layers):\n",
        "    super(EncoderRNN, self).__init__()\n",
        "    self.num_layers = num_layers\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "    self.rnn = rnn_type(hidden_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "  def forward(self, input, hidden):\n",
        "\n",
        "    embedded = self.embedding(input).view(1, 1, -1)\n",
        "    output, hidden = self.rnn(embedded, hidden)\n",
        "    return output, hidden\n",
        "\n",
        "  def initHidden(self):\n",
        "    return torch.zeros(self.num_layers, 1, self.hidden_size, device=device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-23T07:41:09.217556Z",
          "iopub.execute_input": "2023-06-23T07:41:09.217946Z",
          "iopub.status.idle": "2023-06-23T07:41:09.225030Z",
          "shell.execute_reply.started": "2023-06-23T07:41:09.217916Z",
          "shell.execute_reply": "2023-06-23T07:41:09.224157Z"
        },
        "trusted": true,
        "id": "fChPkMIlrbLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder"
      ],
      "metadata": {
        "id": "UNLzCMjbrbLo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "  def __init__(self, hidden_size, output_size, rnn_type, num_layers, dropout=0.1, max_length=MAX_LENGTH):\n",
        "    super(DecoderRNN, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    self.num_layers = num_layers\n",
        "    self.dropout_p = dropout\n",
        "    self.max_length = max_length\n",
        "\n",
        "    self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "\n",
        "    self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "    self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "    self.dropout = nn.Dropout(self.dropout_p)\n",
        "\n",
        "    self.rnn = rnn_type(self.hidden_size, self.hidden_size, self.num_layers, batch_first=True)\n",
        "    self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
        "#     self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "  def forward(self, input, hidden, encoder_outputs):\n",
        "    embedded = self.embedding(input).view(1, 1, -1)\n",
        "    embedded = self.dropout(embedded)\n",
        "\n",
        "    attn_weights = F.softmax(\n",
        "        self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "\n",
        "    attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                            encoder_outputs.unsqueeze(0))\n",
        "\n",
        "    output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "    output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "    output = F.relu(output)\n",
        "    output, hidden = self.rnn(output, hidden)\n",
        "    output = F.log_softmax(self.fc(output[0]), dim=1)\n",
        "    return output, hidden, attn_weights\n",
        "\n",
        "  def initHidden(self):\n",
        "    return torch.zeros(self.num_layers, 1, self.hidden_size, device=device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-23T08:18:15.243096Z",
          "iopub.execute_input": "2023-06-23T08:18:15.243534Z",
          "iopub.status.idle": "2023-06-23T08:18:15.257263Z",
          "shell.execute_reply.started": "2023-06-23T08:18:15.243499Z",
          "shell.execute_reply": "2023-06-23T08:18:15.256167Z"
        },
        "trusted": true,
        "id": "X2W4XBvkrbLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder scalar"
      ],
      "metadata": {
        "id": "kWH8XaqGrbLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderRNN_scalar(nn.Module):\n",
        "  def __init__(self, hidden_size, output_size, rnn_type, num_layers, dropout=0.1, max_length=MAX_LENGTH):\n",
        "    super(DecoderRNN_scalar, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    self.num_layers = num_layers\n",
        "    self.dropout_p = dropout\n",
        "    self.max_length = max_length\n",
        "\n",
        "    self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "\n",
        "    self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "    self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "    self.dropout = nn.Dropout(self.dropout_p)\n",
        "\n",
        "    self.rnn = rnn_type(self.hidden_size, self.hidden_size, self.num_layers, batch_first=True)\n",
        "    self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
        "#     self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "  def forward(self, input, hidden, encoder_outputs):\n",
        "    embedded = self.embedding(input).view(1, 1, -1)\n",
        "    embedded = self.dropout(embedded)\n",
        "\n",
        "    attn_weights = F.softmax( (embedded[0] @ encoder_outputs.T) / self.max_length**0.5, dim=1)\n",
        "\n",
        "    attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                            encoder_outputs.unsqueeze(0))\n",
        "\n",
        "    output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "    output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "    output = F.relu(output)\n",
        "    output, hidden = self.rnn(output, hidden)\n",
        "    output = F.log_softmax(self.fc(output[0]), dim=1)\n",
        "    return output, hidden, attn_weights\n",
        "\n",
        "  def initHidden(self):\n",
        "    return torch.zeros(self.num_layers, 1, self.hidden_size, device=device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-23T09:51:09.090129Z",
          "iopub.execute_input": "2023-06-23T09:51:09.090520Z",
          "iopub.status.idle": "2023-06-23T09:51:09.102154Z",
          "shell.execute_reply.started": "2023-06-23T09:51:09.090489Z",
          "shell.execute_reply": "2023-06-23T09:51:09.101115Z"
        },
        "trusted": true,
        "id": "OkkdJkqJrbLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-23T08:06:02.261504Z",
          "iopub.execute_input": "2023-06-23T08:06:02.262604Z",
          "iopub.status.idle": "2023-06-23T08:06:02.271654Z",
          "shell.execute_reply.started": "2023-06-23T08:06:02.262566Z",
          "shell.execute_reply": "2023-06-23T08:06:02.270525Z"
        },
        "trusted": true,
        "id": "xlJNHxSHrbLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "teacher_forcing = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, rnn_type, num_layers, max_length=MAX_LENGTH):\n",
        "\n",
        "  if rnn_type.__name__ == 'GRU':\n",
        "      encoder_hidden = encoder.initHidden()\n",
        "  else:\n",
        "      encoder_hidden = (encoder.initHidden(), encoder.initHidden())\n",
        "\n",
        "  encoder_optimizer.zero_grad()\n",
        "  decoder_optimizer.zero_grad()\n",
        "\n",
        "  input_length = input_tensor.size(0)\n",
        "  target_length = target_tensor.size(0)\n",
        "\n",
        "  encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "  loss = 0\n",
        "\n",
        "  for ei in range(input_length):\n",
        "    encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "    encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "  decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "  decoder_hidden = encoder_hidden\n",
        "\n",
        "  use_teacher_forsing = True if random.random() < teacher_forcing else False\n",
        "\n",
        "  if use_teacher_forsing:\n",
        "    for di in range(target_length):\n",
        "      decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "      loss += criterion(decoder_output, target_tensor[di])\n",
        "\n",
        "      decoder_input = target_tensor[di]\n",
        "\n",
        "  else:\n",
        "    for di in range(target_length):\n",
        "      decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "      loss += criterion(decoder_output, target_tensor[di])\n",
        "\n",
        "      topv, topi = decoder_output.topk(1)\n",
        "      decoder_input = topi.squeeze().detach()\n",
        "      if decoder_input.item() == EOS_token:\n",
        "        break\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  encoder_optimizer.step()\n",
        "\n",
        "  decoder_optimizer.step()\n",
        "\n",
        "  return loss.item() / target_length"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-23T08:08:52.618595Z",
          "iopub.execute_input": "2023-06-23T08:08:52.618980Z",
          "iopub.status.idle": "2023-06-23T08:08:52.631063Z",
          "shell.execute_reply.started": "2023-06-23T08:08:52.618951Z",
          "shell.execute_reply": "2023-06-23T08:08:52.630071Z"
        },
        "trusted": true,
        "id": "9n5BYxuHrbLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-23T08:09:09.316239Z",
          "iopub.execute_input": "2023-06-23T08:09:09.316607Z",
          "iopub.status.idle": "2023-06-23T08:09:09.322927Z",
          "shell.execute_reply.started": "2023-06-23T08:09:09.316579Z",
          "shell.execute_reply": "2023-06-23T08:09:09.321757Z"
        },
        "trusted": true,
        "id": "JJ65fT7yrbLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-23T08:09:09.492983Z",
          "iopub.execute_input": "2023-06-23T08:09:09.493642Z",
          "iopub.status.idle": "2023-06-23T08:09:09.500762Z",
          "shell.execute_reply.started": "2023-06-23T08:09:09.493601Z",
          "shell.execute_reply": "2023-06-23T08:09:09.499270Z"
        },
        "trusted": true,
        "id": "5PuEn1HrrbLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trainIters(encoder, decoder, n_iters, rnn_type, num_layers=1, print_every=1000, plot_every=100, learning_rate=0.001):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion, rnn_type, num_layers)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-23T08:09:30.206642Z",
          "iopub.execute_input": "2023-06-23T08:09:30.206998Z",
          "iopub.status.idle": "2023-06-23T08:09:30.217993Z",
          "shell.execute_reply.started": "2023-06-23T08:09:30.206970Z",
          "shell.execute_reply": "2023-06-23T08:09:30.216249Z"
        },
        "trusted": true,
        "id": "yhef7VMBrbLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(encoder, decoder, sentence, rnn_type, num_layers, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "\n",
        "\n",
        "        if rnn_type.__name__ == 'GRU':\n",
        "            encoder_hidden = encoder.initHidden()\n",
        "        else:\n",
        "            encoder_hidden = (encoder.initHidden(), encoder.initHidden())\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-23T08:15:45.506055Z",
          "iopub.execute_input": "2023-06-23T08:15:45.506470Z",
          "iopub.status.idle": "2023-06-23T08:15:45.517078Z",
          "shell.execute_reply.started": "2023-06-23T08:15:45.506438Z",
          "shell.execute_reply": "2023-06-23T08:15:45.516166Z"
        },
        "trusted": true,
        "id": "RXfAvUlLrbLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluateRandomly(encoder, decoder, rnn_type, num_layers, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, _ = evaluate(encoder, decoder, pair[0], rnn_type, num_layers)\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-23T08:45:56.107652Z",
          "iopub.execute_input": "2023-06-23T08:45:56.108024Z",
          "iopub.status.idle": "2023-06-23T08:45:56.114708Z",
          "shell.execute_reply.started": "2023-06-23T08:45:56.107994Z",
          "shell.execute_reply": "2023-06-23T08:45:56.113740Z"
        },
        "trusted": true,
        "id": "swQbYUeirbLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 Layer GRU\n",
        "hidden_size = 256\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size, nn.GRU, 1).to(device)\n",
        "decoder1 = DecoderRNN(hidden_size, output_lang.n_words, nn.GRU, 1).to(device)\n",
        "\n",
        "trainIters(encoder1, decoder1, 100000, nn.GRU, num_layers=1, print_every=5000)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-23T08:18:18.636106Z",
          "iopub.execute_input": "2023-06-23T08:18:18.636479Z",
          "iopub.status.idle": "2023-06-23T08:45:09.722134Z",
          "shell.execute_reply.started": "2023-06-23T08:18:18.636446Z",
          "shell.execute_reply": "2023-06-23T08:45:09.721045Z"
        },
        "trusted": true,
        "id": "B1G0aS8crbLt",
        "outputId": "b7fee3af-4be1-472e-b002-f208fafa595a"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "1m 24s (- 26m 38s) (5000 5%) 4.6315\n2m 43s (- 24m 27s) (10000 10%) 4.2509\n4m 2s (- 22m 53s) (15000 15%) 4.0556\n5m 21s (- 21m 27s) (20000 20%) 3.9162\n6m 41s (- 20m 5s) (25000 25%) 3.7946\n8m 2s (- 18m 44s) (30000 30%) 3.7622\n9m 22s (- 17m 25s) (35000 35%) 3.6847\n10m 42s (- 16m 4s) (40000 40%) 3.6044\n12m 3s (- 14m 44s) (45000 45%) 3.5588\n13m 24s (- 13m 24s) (50000 50%) 3.5064\n14m 45s (- 12m 4s) (55000 55%) 3.4483\n16m 6s (- 10m 44s) (60000 60%) 3.4123\n17m 27s (- 9m 23s) (65000 65%) 3.3665\n18m 47s (- 8m 3s) (70000 70%) 3.3326\n20m 7s (- 6m 42s) (75000 75%) 3.2973\n21m 28s (- 5m 22s) (80000 80%) 3.2432\n22m 48s (- 4m 1s) (85000 85%) 3.1768\n24m 10s (- 2m 41s) (90000 90%) 3.1483\n25m 30s (- 1m 20s) (95000 95%) 3.1383\n26m 50s (- 0m 0s) (100000 100%) 3.0916\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluateRandomly(encoder1, decoder1, nn.GRU, 1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-23T08:45:59.115221Z",
          "iopub.execute_input": "2023-06-23T08:45:59.116090Z",
          "iopub.status.idle": "2023-06-23T08:45:59.188431Z",
          "shell.execute_reply.started": "2023-06-23T08:45:59.116047Z",
          "shell.execute_reply": "2023-06-23T08:45:59.187361Z"
        },
        "trusted": true,
        "id": "CHidskUorbLt",
        "outputId": "a82d15c6-47c4-432b-8e4b-91e26636d227"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "> you re on the list aren t you ?\n= ты же есть в списке ?\n< вы ведь на ? ? <EOS>\n\n> i m sure tom will agree to help .\n= я уверен что том согласится помочь .\n< я уверен что том том . . <EOS>\n\n> i m different from tom .\n= я не такои как том .\n< я уже что том том . <EOS>\n\n> i m not laughing .\n= я не смеюсь .\n< я не не . <EOS>\n\n> we re not finished yet .\n= мы еще не закончили .\n< мы не не не . <EOS>\n\n> he is absorbed in reading detective novels .\n= он погружен в чтение детективов .\n< он в в в . . <EOS>\n\n> i m pretty sure .\n= я вполне уверен .\n< я уверен уверен . <EOS>\n\n> i m sure she ll leave early .\n= я уверена что она рано уидет .\n< я уверен уверен в этом . <EOS>\n\n> i m sure you are wrong .\n= я уверен что ты ошибаешься .\n< я уверен что вы не . <EOS>\n\n> you re hopeless .\n= ты безнадежен !\n< вы хорошии . <EOS>\n\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 Layers GRU\n",
        "hidden_size = 256\n",
        "encoder2 = EncoderRNN(input_lang.n_words, hidden_size, nn.GRU, num_layers=2).to(device)\n",
        "decoder2 = DecoderRNN(hidden_size, output_lang.n_words, nn.GRU, num_layers=2).to(device)\n",
        "\n",
        "trainIters(encoder2, decoder2, 100000, nn.GRU, num_layers=2, print_every=5000)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-23T08:46:11.785775Z",
          "iopub.execute_input": "2023-06-23T08:46:11.786157Z",
          "iopub.status.idle": "2023-06-23T09:15:22.582830Z",
          "shell.execute_reply.started": "2023-06-23T08:46:11.786126Z",
          "shell.execute_reply": "2023-06-23T09:15:22.581819Z"
        },
        "trusted": true,
        "id": "oNAZqdNRrbLt",
        "outputId": "ee60bbce-33f2-4b07-8dae-5562794ffc7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "1m 30s (- 28m 41s) (5000 5%) 4.8070\n2m 55s (- 26m 21s) (10000 10%) 4.2739\n4m 22s (- 24m 47s) (15000 15%) 4.1288\n5m 49s (- 23m 16s) (20000 20%) 4.0151\n7m 16s (- 21m 48s) (25000 25%) 3.9239\n8m 43s (- 20m 21s) (30000 30%) 3.8598\n10m 10s (- 18m 54s) (35000 35%) 3.8195\n11m 38s (- 17m 27s) (40000 40%) 3.7707\n13m 5s (- 16m 0s) (45000 45%) 3.7037\n14m 32s (- 14m 32s) (50000 50%) 3.6506\n16m 0s (- 13m 5s) (55000 55%) 3.6196\n17m 29s (- 11m 39s) (60000 60%) 3.5905\n18m 58s (- 10m 13s) (65000 65%) 3.5466\n20m 25s (- 8m 45s) (70000 70%) 3.4933\n21m 52s (- 7m 17s) (75000 75%) 3.4683\n23m 20s (- 5m 50s) (80000 80%) 3.4126\n24m 47s (- 4m 22s) (85000 85%) 3.3938\n26m 15s (- 2m 55s) (90000 90%) 3.3673\n27m 42s (- 1m 27s) (95000 95%) 3.3220\n29m 9s (- 0m 0s) (100000 100%) 3.2925\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluateRandomly(encoder2, decoder2, nn.GRU, 2)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-23T09:15:22.585003Z",
          "iopub.execute_input": "2023-06-23T09:15:22.585387Z",
          "iopub.status.idle": "2023-06-23T09:15:22.659517Z",
          "shell.execute_reply.started": "2023-06-23T09:15:22.585338Z",
          "shell.execute_reply": "2023-06-23T09:15:22.658504Z"
        },
        "trusted": true,
        "id": "2zBK44t5rbLu",
        "outputId": "c02dbcb2-16d5-4373-8b26-e8c0ef73d30e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "> she s sure to succeed .\n= к неи непременно придет успех .\n< она очень это . . <EOS>\n\n> i m starting to lose hope .\n= я начинаю терять надежду .\n< я собираюсь в . . <EOS>\n\n> i m not good at doing that .\n= у меня это не очень хорошо получается .\n< я не не в в . . <EOS>\n\n> i m a bit shy .\n= я немного застенчив .\n< я в . . <EOS>\n\n> you re completely healthy .\n= ты совершенно здоров .\n< вы слишком . . <EOS>\n\n> she s worried about your safety .\n= она волнуется за твою безопасность .\n< она на на на . . <EOS>\n\n> i m trying to concentrate .\n= я пытаюсь сосредоточиться .\n< я собираюсь . . <EOS>\n\n> you re a little weird .\n= ты немного странныи .\n< ты хорошии . . <EOS>\n\n> i m happy to see you here .\n= я рад видеть вас здесь .\n< я так рад что ты . . <EOS>\n\n> you re not my friend .\n= ты мне не друг .\n< вы не не . . <EOS>\n\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 Layer GRU scalar\n",
        "hidden_size = 256\n",
        "encoder3 = EncoderRNN(input_lang.n_words, hidden_size, nn.GRU, num_layers=1).to(device)\n",
        "decoder3 = DecoderRNN_scalar(hidden_size, output_lang.n_words, nn.GRU, num_layers=1).to(device)\n",
        "\n",
        "trainIters(encoder3, decoder3, 100000, nn.GRU, num_layers=2, print_every=5000)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-23T09:51:13.746489Z",
          "iopub.execute_input": "2023-06-23T09:51:13.746848Z",
          "iopub.status.idle": "2023-06-23T10:17:37.537210Z",
          "shell.execute_reply.started": "2023-06-23T09:51:13.746818Z",
          "shell.execute_reply": "2023-06-23T10:17:37.536164Z"
        },
        "trusted": true,
        "id": "zgYiUE4qrbLu",
        "outputId": "8fa03e8d-0c76-4a1f-d47c-c5b445e5c4ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "1m 22s (- 26m 2s) (5000 5%) 4.6154\n2m 40s (- 24m 0s) (10000 10%) 4.2843\n3m 58s (- 22m 30s) (15000 15%) 4.1346\n5m 16s (- 21m 6s) (20000 20%) 4.0022\n6m 34s (- 19m 43s) (25000 25%) 3.8671\n7m 53s (- 18m 24s) (30000 30%) 3.7766\n9m 12s (- 17m 5s) (35000 35%) 3.7245\n10m 32s (- 15m 48s) (40000 40%) 3.6547\n11m 52s (- 14m 31s) (45000 45%) 3.5895\n13m 11s (- 13m 11s) (50000 50%) 3.5443\n14m 30s (- 11m 52s) (55000 55%) 3.4943\n15m 50s (- 10m 33s) (60000 60%) 3.4526\n17m 9s (- 9m 14s) (65000 65%) 3.3983\n18m 28s (- 7m 55s) (70000 70%) 3.3778\n19m 47s (- 6m 35s) (75000 75%) 3.3181\n21m 6s (- 5m 16s) (80000 80%) 3.2830\n22m 25s (- 3m 57s) (85000 85%) 3.2756\n23m 43s (- 2m 38s) (90000 90%) 3.1765\n25m 3s (- 1m 19s) (95000 95%) 3.1902\n26m 22s (- 0m 0s) (100000 100%) 3.1123\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluateRandomly(encoder3, decoder3, nn.GRU, 1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-23T10:21:23.014414Z",
          "iopub.execute_input": "2023-06-23T10:21:23.015128Z",
          "iopub.status.idle": "2023-06-23T10:21:23.089714Z",
          "shell.execute_reply.started": "2023-06-23T10:21:23.015091Z",
          "shell.execute_reply": "2023-06-23T10:21:23.088655Z"
        },
        "trusted": true,
        "id": "kH4YuYUIrbLu",
        "outputId": "df5921e7-d947-41cc-ae51-6fb85d18d11d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "> i m not sure that i m right .\n= я не уверен что прав .\n< я не уверен что я . <EOS>\n\n> you re going to ruin your eyes .\n= вы себе глаза испортите .\n< ты в в . . <EOS>\n\n> i m glad tom was caught .\n= я рад что тома поимали .\n< я рад что том это . <EOS>\n\n> i m very grateful .\n= я очень благодарна .\n< я очень хорошо . <EOS>\n\n> we re able to do that .\n= мы в состоянии с этим справиться .\n< мы будем это . . <EOS>\n\n> you re on the wrong road .\n= вы неправильно едете .\n< ты не в . . <EOS>\n\n> we re coming back tonight .\n= мы возвращаемся сегодня вечером .\n< мы оба в . . <EOS>\n\n> i m not a good cook .\n= я не очень хорошо готовлю .\n< я не очень хорошо . <EOS>\n\n> i m starting a new life .\n= я начинаю новую жизнь .\n< я немного на . . <EOS>\n\n> i m tired of translating .\n= мне надоело переводить .\n< я устал устал . <EOS>\n\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 Layers GRU scalar\n",
        "hidden_size = 256\n",
        "encoder4 = EncoderRNN(input_lang.n_words, hidden_size, nn.GRU, num_layers=2).to(device)\n",
        "decoder4 = DecoderRNN_scalar(hidden_size, output_lang.n_words, nn.GRU, num_layers=2).to(device)\n",
        "\n",
        "trainIters(encoder4, decoder4, 100000, nn.GRU, num_layers=2, print_every=5000)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-23T10:21:27.388525Z",
          "iopub.execute_input": "2023-06-23T10:21:27.388883Z",
          "iopub.status.idle": "2023-06-23T10:50:07.740635Z",
          "shell.execute_reply.started": "2023-06-23T10:21:27.388854Z",
          "shell.execute_reply": "2023-06-23T10:50:07.739606Z"
        },
        "trusted": true,
        "id": "nxGwNqH5rbLu",
        "outputId": "4af10344-8490-463b-e65f-698901442f4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "1m 26s (- 27m 28s) (5000 5%) 4.7221\n2m 49s (- 25m 22s) (10000 10%) 4.2679\n4m 14s (- 24m 0s) (15000 15%) 4.1761\n5m 38s (- 22m 35s) (20000 20%) 4.0622\n7m 4s (- 21m 13s) (25000 25%) 3.9572\n8m 30s (- 19m 50s) (30000 30%) 3.8751\n9m 55s (- 18m 26s) (35000 35%) 3.8170\n11m 21s (- 17m 2s) (40000 40%) 3.7440\n12m 49s (- 15m 39s) (45000 45%) 3.7028\n14m 16s (- 14m 16s) (50000 50%) 3.6457\n15m 43s (- 12m 51s) (55000 55%) 3.6290\n17m 8s (- 11m 25s) (60000 60%) 3.5543\n18m 34s (- 10m 0s) (65000 65%) 3.5459\n20m 1s (- 8m 34s) (70000 70%) 3.4963\n21m 27s (- 7m 9s) (75000 75%) 3.4742\n22m 53s (- 5m 43s) (80000 80%) 3.3994\n24m 20s (- 4m 17s) (85000 85%) 3.3766\n25m 47s (- 2m 51s) (90000 90%) 3.3586\n27m 13s (- 1m 25s) (95000 95%) 3.3047\n28m 39s (- 0m 0s) (100000 100%) 3.2769\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluateRandomly(encoder4, decoder4, nn.GRU, 2)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-23T10:50:07.742661Z",
          "iopub.execute_input": "2023-06-23T10:50:07.743005Z",
          "iopub.status.idle": "2023-06-23T10:50:07.821222Z",
          "shell.execute_reply.started": "2023-06-23T10:50:07.742972Z",
          "shell.execute_reply": "2023-06-23T10:50:07.820144Z"
        },
        "trusted": true,
        "id": "0PSQ4Eo6rbLu",
        "outputId": "42b8ba08-f03d-403e-de65-4150ae8cd49c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "> i m not that wild .\n= я не настолько буиныи .\n< я не не . . <EOS>\n\n> i m sure a solution can be found .\n= я уверен что решение может быть наидено .\n< я уверен что том не . . <EOS>\n\n> i m not ready to die .\n= я не готов умирать .\n< я не собираюсь не . <EOS>\n\n> we re almost ready .\n= мы почти готовы .\n< мы все . . <EOS>\n\n> i m quite young .\n= я довольно молод .\n< я не устал . <EOS>\n\n> i am not coming today .\n= я сегодня не приду .\n< я не не . . <EOS>\n\n> i m looking for a sweater .\n= я ищу свитер .\n< я ищу . <EOS>\n\n> i m tired of losing .\n= я устал проигрывать .\n< я устал в . <EOS>\n\n> i m sorry i forgot your name .\n= простите я забыл ваше имя .\n< я что я я . . . <EOS>\n\n> i m glad you didn t do it .\n= я рад что вы этого не сделали .\n< я рад что ты не не . . <EOS>\n\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ypxHE-jTrbLv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}